추론 기반 기법에는 어떠한 모델이 등장한다.
우리는 이 모델로 신경망을 사용한다.
모델은 맥락 정보를 입력받아 출현할 수 있는 각 단어의 출현 확률을 출력한다.
이러한 틀 안에서 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습시킨다.
그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림이다.

추론 기반 기법도 통계 기반 기법처럼 분포 가설에 기초한다.
분포 가설이란, 단어의 의미는 주변 단어에 의해 형성된다는 가설로, 이를 추측 문제로 귀결시켰다.
이처럼 두 기법 모두 분표 가설에 근거하는 단어의 동시발생 가능성을 얼마나 잘 모델링하는가가 중요한 연구 주제이다.

- 완전연결계층에 의한 변환
(입력층7, 은닉층3)
화살표에는 가중치:매개변수가 존재하여, 입력층 뉴런과의 가중합이 은닉층 뉴런이 된다.
편향을 이용하지 않은 완전연결계층은 행렬 곱 계산에 해당한다.

모델을 신경망으로 구축하는 것
신경망은 word2vec에서 제안하는 cbow 모델이다.

cbow 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.
타깃은 중앙 단어이고, 그 주변 단어들이 맥락이다.

-- 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문
은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데, 입력층이 여러 개이면 전체를 평균하면 된다.
출력층의 뉴런은 총 7개인데, 중요한 것은 이 뉴런 하나하나가 각각의 단어에 대응한다는 점이다.
그리고 출력층 뉴런은 각 단어의 점수를 뜻하며, 값이 높을수록 대응 단어의 출현 확률도 높아진다.
여기서 점수란, 확률로 해석되기 전의 값이고, 이 점수에 소프트맥스 함수를 적용해서 확률을 얻을 수 있다.
점수를 Softmax 계층에 통과시킨 후의 뉴런을 출력층이라고도 한다.

은닉층의 뉴런 수를 입력 층의 뉴런 수보다 적게 하는 것이 중요한 핵심이다.
이렇게 해야 은닉층에는 단어 예측에 필요한 정보를 간결하게 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있다.
이 때 은닉층 정보는 인간이 이해할 수 없는 코드로 쓰여있다. (인코딩)
한편, 은닉층의 정보로부터 원하는 결과를 얻는 작업은 디코딩이라고 한다.
즉, 디코딩이란 인코딩된 정보를 인간이 이해할 수 있는 표현으로 복원하는 작업이다.

신경망의 학습에 대해 생각해보자
우리가 다루는 모델은 다중 클래스 분류를 수행하는 신경망이다.
따라서 이 신경망을 학습하려면, 소프트맥스 함수와 교차 엔트로피 오차만 이용하면 된다.

소프트맥스를 이용해 점수를 확률로 변환하고,
그 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후,
그 값을 손실로 사용해 학습을 진행한다.

추론 처리를 수행하는 CBOW 모델에 Softmax 계층과 Cross Entropy 계층을 추가한 것만으로도 손실을 얻을 수 있다.
이상이 CBOW 모델의 손실을 구하는 계산 흐름이자, 이 신경망의 순방향 전파이다.
Softmax 계층과 Cross Entropy Error 계층은 Softmax with Loss 계층 하나로 구현할 수 있다.

학습 코드 구현
CBOW 모델의 학습은 일반적인 신경망의 학습과 완전히 같다.
학습 데이터를 준비해 신경망에 입력한 다음,
기울기를 구하고 가중치 매개변수를 순서대로 갱신해간다.
매개변수 갱신 방법: SGD, AdaGrad 등 중 Adam 을 선택했다.
Train 클래스는 신경망을 학습시킨다.
학습 데이터로부터 미니배치를 선택한 다음, 신경망에 입력해 기울기를 구하고, 그 기울기를 Optimizer 에 넘겨 매개변수를 갱신하는 일련의 작업을 수행한다.

