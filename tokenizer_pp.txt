한국어 모델링의 본격적인 시작인 Tokenizer
한국어를 위한 다양한 Tokenizer를 알아보고, 특히 대화체와 가장 합이 잘 맞는 Tokenizer가 무엇인지 실험을 통해 알아보자(나는 대화체? 채팅보단 구어체임)

유한한 단어 (word) 혹은 글자 (character) 단위로 문장을 쪼개고 각각을 숫자로 매핑시켜서 sequence 모델링을 합니다.
이 때 각각의 쪼개진 단위를 token 이라고 하고, 문자을 적절한 token으로 자르는 작업을 tokenize 라고 합니다.

그렇다면 tokenize는 어떻게 하는게 좋을까
이상적인 tokenize 결과는 각 token이 의미 단위로 적절히 쪼개지는 것이라고 할 수 있다.
적절한 의미 단위의 tokenize를 위해 간단히 공백을 기준으로 자를 수도 있고 형태소 분석기를 이용해서 문법적 의미 단위를 잡아낼 수도 있을 것
또한 최근에는 Subword Unit 의 통계 정보를 기반으로 한 Word Piece Model 도 많이 사용되고 있음

한국어 대화체에 적절한 Tokenizer? 
영어와 한국어는 의미 단위를 구분하는 데이터 처리 관점에서 많은 차이점이 있습니다. 영어는 띄어쓰기 단위로 각 단어가 비교적 명확한 의미를 갖기 때문에 구분이 쉽지만, 한국어는 어미의 변형으로 인해
의미 단위를 쉽게 구분하기 어렵습니다.

Tokenizers
형태소 분석기 기반 tokenizer
Word Piece Model 로 대표되는 Byte Pair Encoding (BPE) 기반의 tokenize

BPE 기반 Tokenizer
이런 OOV 문제를 해결하기 위해서 최근에는 데이터 압축 기법인 BPE 알고리즘을 기반으로 Subword Unit에 대한 사전을 만드는 기법이 많이 활용
빈도수가 높은 Subword Unit들을 병합시켜나가면서 최종적으로 자주 나오는 단어는 그 자체를 그대로 저장하고 덜 나오는 단어는 subword로 나누어서 저장하므로서 효울적인 사전 관리를 통해
OOV를 줄이는 목적을 갖고 있습니다.
