자연어 형태의 텍스트를 컴퓨터가 이해하게 하기 위해서는 토크나이징 (tokenizing) 과정이 필수적으로 진행되어야 합니다.
그러나 핑퐁팀에서 다루고 있는 채팅 데이터는 띄어쓰기가 제대로 안 된 데이터가 많고 이로 인해 토크나이징 단계에서 오류가 발생하기도 합니다.
어떻게 하면 채팅 문장의 띄어쓰기를 수정할 수 있을까요?

1. 문제 소개
우리가 사용하는 텍스트를 컴퓨터가 이해하게 용이하게 작은 단위로 쪼개는 작업을 토크나이징이라고 합니다.
문장이 제대로 토크나이징되려면 그 문장의 띄어쓰기가 잘 이루어져야 합니다.
그러나 채팅 문장에서는 띄어쓰기를 생략하는 경우가 많으며, 이 때 토크나이징 결과가 올바르게 출력되지 않기도 합니다.

가장 좋은 방법은 문장을 맞춤법에 맞춰 수정하는 것입니다. 문장의 맞춤법을 교정하면 아래와 같이 토크나이징 결과가 올바르게 수정되는 것을 볼 수 있습니다.
그러나 채팅에서만 사용되는 어휘나 표현의 특성을 잃을 수 있고, 맞춤법 전체를 교정하는 모델을 만드는 것은 너무 복잡한 일.

우리는 문장의 띄어쓰기를 교정한다면 토크나이징 성능을 향상 시킬 수 있다는 가정을 세웠습니다.
문장이 올바르게 토크나이징 되지 않는 이유는 여러가지가 있지만, 그 중 띄어쓰기는 단어를 구분하는 기준으로 사용되기 때문에 토크나이징에 영향을 줄 수
있다고 생각했습니다.
이를 위해 채팅 데이터에 적합한 띄어쓰기 모델을 구현하고자 했습니다.

띄어쓰기 오류가 빈번한 채팅 데이터에서 띄어쓰기 오류를 수정하는 모델의 구현 과정을 설명합니다.
우리가 구현한 모델의 성능을 측정하기 위해 현존하는 띄어쓰기 및 맞춤법 교정기들과 비교 실험하고 그 결과를 분석합니다.
띄어쓰기 모델을 통해 띄어쓰기를 교정한 문장의 토크나이징 결과가 이전에 비해 얼마나 향상되는지 확인합니다.

한국어 채팅 데이터의 경우
이처럼 띄어쓰기는 가독성 뿐만 아니라 의미 전달에 중요한 역할을 하지만, 스캐터랩에서 수집한 채팅 데이터에서는 띄어쓰기가 생략된 문장을
빈번히 발견할 수 있었습니다. 사람들이 띄어쓰기를 안 하는 이유는 무엇일까요? 우리는 그 이유를 크게 두 가지로 보았습니다.
1. 가독성 보단 속도
채팅이라는 특성상 상대방에게 자신의 의사를 빨리 전달하는 것을 우선시하게 됩니다. 그렇기 때문에 시간이 오래 드는 띄어쓰기를 생략하여 더 빨리
상대방에게 채팅을 전달하기 때문이라고 보았습니다. 귀찮다는 이유도 있죠.
2. 띄어쓰기가 의미의 영향을 주지 않아서
두번째로는 띄어쓰기의 영향이 적은 구나 문장의 경우 띄어쓰기를 생략하는 경우입니다. 띄어쓰기 실수 중 가장 많은 경우가 "~을/를 안 하다 (좋다/사다/가다 등)"
를 "~을/를 안하다 (안좋다/안사다/안가다 등)"로 쓰는 경우 입니다. 작성자가 올바른 띄어쓰기 방법을 알고 있다 하더라도 띄어쓰기의 여부가 문장의 의미나
의도 전달에 영향을 주지 않기 때문에 생략하는 경우가 대부분이며, 실제로 이로 인한 의사소통 문자게 발생하지 않기 때문에 문제시 되지 않고 있습니다.

요약
위에서는 한국어에서의 띄어쓰기의 특성 및 채팅 데이터에서 띄어쓰기가 제대로 지켜지지 않는 이유에 대해서 알아보았습니다.
사람들이 의사도통을 할 때에는 띄어쓰기를 생략해도 의미 전달에 크게 문제가 없으나, 이러한 문장을 데이터로써 처리할 때 문제가 발생할 수 있습니다.
이런 문제를 해결하고자 채팅 데이터의 띄어쓰기 특성을 반영하는 모델을 학습하여 문장의 띄어쓰기를 수정하여 처리하고자 합니다.

3. 모델 조사 및 코딩
기존 모델 조사
한국어의 띄어쓰기를 교정하는 오픈소스 모델에는 KoSpacing과 RAWS가 있으며, 둘 다 CNN RNN으로 이루어진 모델입니다.
두 모델의 차이점으로 KoSpacing은 CNN과 RNN을 직렬로 연결하였고, RAWS는 이들을 병렬로 연결하여 학습하였다는 점이 있습니다.

KoSpacing은 n-gram의 정보와 단어의 시퀀스 정보를 활용하여 문장의 띄어쓰기를 교정하도록 신경망을 구성한 모델입니다.
n-gram의 특성을 반영하기 위해 각각 다른 윈도우 크기를 가지는 CNN들을 모델링 하였고, 이를 전부 합친 후 (concatenation) 여러 개의 CNN이
압축한 정보를 RNN을 통해 순차적으로 모델링하였습니다.
Fasttext 방법으로 기학습된 단어 임베딩을 사용하였기 때문에, 임베딩 사전에 포함되어 있지 않은 단어의 경우 "<unk>" 토큰으로 대체되는 문제점이 있습니다.

RAWS는 문자 단위로 토크나이징을 하여 모든 문자에 대한 임베딩을 생성하였으며, 문자 임베딩의 시퀀스를 각 CNN과 RNN으로 모델링한 후 합치는 구조를
가지고 있습니다. 두 가지의 다른 특성을 앙상블하는 아이디어를 사용하여 표현력은 KoSpacing보다 우수하지만, CNN과 RNN 사이에 병목 현상이 발생하여
실시간 입출력을 구현하기 어려운 구조입니다.
또한 학습 시 띄어쓰기가 제거된 문장만을 입력하도록 문제를 설정하였기 때문에, 테스트 시 기존 문장의 띄어쓰기를 활용하지 못 하는 문제점이 있습니다.

모델 설계 과정
우리는 기존 모델들의 장점을 취하여 더 좋은 모델 구조를 구성하고자 했습니다.
KoSpacing에서는 CNN-RNN 구조를, RAWS에서는 문자 수준 임베딩 기법을 차용하여 기본적인 모델을 구성하였습니다.
KoSpacing 모델은 길이가 긴 텍스트를 고려하여 여러 개의 CNN을 병렬로 구성하였지만, 채팅 데이터는 길이가 매우 짧기 때문에 여러개의 CNN을 직렬로 연결하였습니다.
추가로 계층적 (hierarchcal) 정보를 반영하기 위해 각 CNN 레이어의 출력을 이어 붙인 (concatenate) 벡터를 다음 CNN의 출력으로 넘겨주었습니다.
CNN 이후에는 각 데이터의 순서 정보를 반영하기 위해 RNN을 사용하였고, 학습 속도를 향상시키기 위하여 CNN 뒤에 BatchNorm 레이어를, RNN 뒤에는 
LayerNorm 레이어를 추가하였습니다.

결론
정리
지금까지 띄어쓰기 오류가 많은 채팅 데이터를 제대로 토크나이징하기 위한 띄어쓰기 모델의 구현 과정에 대해 설명하였습니다.
이 모델을 사용하여 채팅 데이터나 구어체 텍스트의 띄어쓰기를 교정하거나 토크나이징 성능을 향상시키는데 사용할 수 있습니다.
또한 실험 및 분석을 통하여 약 2.5 ~ 7.5% 이상의 문장의 토크나이징 성능이 향상되는 것을 검증하였습니다.

이후 과제
앞에서 띄어쓰기 모델이 토크나이징 결광 악영향을 주는 단어는 주로 고유명사나 외래어 등의 저빈도 단어를 포함하는 것을 알 수 있었습니다.
이러한 경우에 띄어쓰기 모델이 잘 못 띄어쓰게 된다면 앞서 언급한 것처럼 이후 토크나이징에 큰 영향을 미칠 수 있습니다.
